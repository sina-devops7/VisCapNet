import gradio as gr
import numpy as np
from PIL import Image
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.preprocessing import image as keras_image

from src.caption_utils import predict_caption

# efficientnet Ù…Ø¯Ù„ feature extractor Ø±Ø§ ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†
effnet = EfficientNetB0(include_top=False, weights="imagenet", pooling="avg")


def extract_features_pil(img_pil):
    img = img_pil.resize((224, 224))
    img_array = keras_image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)
    feature_vector = effnet.predict(img_array)
    return feature_vector


def generate_caption_gradio(
    image, model, tokenizer, max_length
):
    try:
        feature = extract_features_pil(image)
        predicted = predict_caption(model, feature, tokenizer, max_length)
        return f"ğŸ¤– **Predicted Caption:**\n{predicted}"
    except Exception as e:
        return f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±: {str(e)}"


def launch_gradio_app(model, tokenizer, max_length):
    gr.Interface(
        fn=lambda image: generate_caption_gradio(
            image, model, tokenizer, max_length),
        inputs=gr.Image(type="pil"),
        outputs="text",
        title="Image Captioning - Gradio + EfficientNet",
        description="Ù‡Ø± Ø¹Ú©Ø³ÛŒ Ø±Ùˆ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù† Ùˆ Ú©Ù¾Ø´Ù† ØªÙˆÙ„ÛŒØ¯â€ŒØ´Ø¯Ù‡ ØªÙˆØ³Ø· Ù…Ø¯Ù„ Ø±Ùˆ Ø¨Ø¨ÛŒÙ†!",
    ).launch(debug=True, share=True)
