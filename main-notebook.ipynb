{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0fd9bad",
   "metadata": {},
   "source": [
    "# VisCapNet: Complete Image Captioning Pipeline\n",
    "This notebook runs **all steps of the modular VisCapNet project** – from feature extraction to BLEU evaluation and launching the Gradio UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa118ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies (run just once)\n",
    "!pip install gradio tensorflow keras pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a388d5",
   "metadata": {},
   "source": [
    "## 1. Import Modules & Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gradio_app import launch_gradio_app\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from src.kaggle_download import download_flickr8k\n",
    "from src.config import BASE_DIR, WORKING_DIR\n",
    "from src.feature_extractor import (\n",
    "    load_efficientnetb0,\n",
    "    extract_features,\n",
    "    save_features,\n",
    "    load_features\n",
    ")\n",
    "from src.data_processing import (\n",
    "    load_captions_file,\n",
    "    build_image_caption_mapping,\n",
    "    clean,\n",
    "    build_all_captions_list\n",
    ")\n",
    "from src.tokenizer_prepare import (\n",
    "    load_glove_embeddings,\n",
    "    create_tokenizer,\n",
    "    get_vocab_size,\n",
    "    get_max_length,\n",
    "    create_embedding_matrix\n",
    ")\n",
    "from src.dataset import train_test_split\n",
    "from src.data_generator import data_generator\n",
    "from src.model import build_captioning_model, visualize_model\n",
    "from src.callbacks import CustomModelCheckpoint\n",
    "from src.training import train_model\n",
    "from src.caption_utils import predict_caption\n",
    "from src.evaluation import evaluate_bleu\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c25d205",
   "metadata": {},
   "source": [
    "## 2. (Optional) Download Flickr8k Dataset\n",
    "Uncomment and run if you don’t have the dataset ready in `/content/flickr8k`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee263347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_flickr8k()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44332b",
   "metadata": {},
   "source": [
    "## 3. Image Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3511712",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = os.path.join(WORKING_DIR, \"features.pkl\")\n",
    "if not os.path.exists(features_path):\n",
    "    print(\"[INFO] Extracting visual features from images...\")\n",
    "    model_effnet = load_efficientnetb0()\n",
    "    features = extract_features(model_effnet, os.path.join(BASE_DIR, \"Images\"))\n",
    "    save_features(features, features_path)\n",
    "    print(f\"[INFO] Feature extraction completed. Saved to {features_path}\")\n",
    "else:\n",
    "    print(f\"[INFO] Loading image features from {features_path} ...\")\n",
    "    features = load_features(features_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a9dee",
   "metadata": {},
   "source": [
    "## 4. Captions Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c5b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Loading and processing captions...\")\n",
    "captions_doc = load_captions_file(os.path.join(BASE_DIR, \"captions.txt\"))\n",
    "mapping = build_image_caption_mapping(captions_doc)\n",
    "clean(mapping)\n",
    "all_captions = build_all_captions_list(mapping)\n",
    "print(f\"[INFO] Total images: {len(mapping)} | Total cleaned captions: {len(all_captions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aca2b4",
   "metadata": {},
   "source": [
    "## 5. Load GloVe Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Loading GloVe embeddings...\")\n",
    "EMBEDDING_DIM = 100\n",
    "glove_path = os.path.join(WORKING_DIR, \"glove.6B.100d.txt\")\n",
    "embeddings_index = load_glove_embeddings(glove_path, EMBEDDING_DIM)\n",
    "print(f\"[INFO] Loaded GloVe vectors: {len(embeddings_index)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a07cd4",
   "metadata": {},
   "source": [
    "## 6. Tokenizer and Embedding Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Creating tokenizer and building embedding matrix...\")\n",
    "tokenizer = create_tokenizer(all_captions, num_words=10000)\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "max_length = get_max_length(all_captions)\n",
    "embedding_matrix = create_embedding_matrix(tokenizer, embeddings_index, EMBEDDING_DIM)\n",
    "print(f\"[INFO] Vocabulary size: {vocab_size} | Max caption length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e6f4d",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb579be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Splitting dataset into train and test sets...\")\n",
    "train_keys, test_keys = train_test_split(mapping, train_ratio=0.9)\n",
    "print(f\"[INFO] Train images: {len(train_keys)} | Test images: {len(test_keys)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00c183",
   "metadata": {},
   "source": [
    "## 8. Build the Captioning Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Building the image captioning model...\")\n",
    "model = build_captioning_model(vocab_size, max_length, embedding_matrix, EMBEDDING_DIM, trainable_embed=True)\n",
    "visualize_model(model, os.path.join(WORKING_DIR, 'model.png'))\n",
    "print(\"[INFO] Model architecture image saved as model.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cecfc25",
   "metadata": {},
   "source": [
    "## 9. (Optional) Model Training\n",
    "Set `if False:` to `if True:` to enable training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79aef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "steps = len(train_keys) // BATCH_SIZE\n",
    "checkpoint = CustomModelCheckpoint(\n",
    "    save_path=os.path.join(WORKING_DIR, 'best_model.h5'),\n",
    "    monitor='loss',\n",
    "    save_best_only=True,\n",
    "    save_freq=5\n",
    ")\n",
    "# Change to True if you want to train:\n",
    "if False:\n",
    "    print(\"[INFO] Starting model training...\")\n",
    "    train_model(\n",
    "        model, train_keys, mapping, features,\n",
    "        tokenizer, max_length, vocab_size,\n",
    "        BATCH_SIZE, EPOCHS, checkpoint\n",
    "    )\n",
    "    model.save(os.path.join(WORKING_DIR, 'best_model.keras'))\n",
    "    print(\"[INFO] Training completed and model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91830f2b",
   "metadata": {},
   "source": [
    "## 10. Load Trained Model (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc00cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(WORKING_DIR, 'best_model.keras')\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"[INFO] Loading trained model from {model_path} ...\")\n",
    "    model = load_model(model_path)\n",
    "else:\n",
    "    print(\"[WARNING] Trained model not found. Using the current (possibly untrained) model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a89d40",
   "metadata": {},
   "source": [
    "## 11. BLEU Score Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d545586",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Evaluating BLEU scores on the test set...\")\n",
    "bleu_scores = evaluate_bleu(model, features, tokenizer, mapping, test_keys, max_length)\n",
    "for name, score in bleu_scores.items():\n",
    "    print(f\"[RESULT] {name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540a47f",
   "metadata": {},
   "source": [
    "## 12. Inference on a Sample Test Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de99c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = test_keys[0]\n",
    "print(\"\\n[INFO] Inference on a single sample test image:\")\n",
    "print(\"Image ID:\", sample_id)\n",
    "print(\"Ground Truth Captions:\", mapping[sample_id])\n",
    "pred_caption = predict_caption(model, features[sample_id], tokenizer, max_length)\n",
    "print(\"Predicted Caption:\", pred_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abef893",
   "metadata": {},
   "source": [
    "## 13. Launch Gradio Web User Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Launching Gradio Web UI...\")\n",
    "launch_gradio_app(model, tokenizer, max_length)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
